name: BackendAPI

on:
  workflow_dispatch:
  push:
    # branches:
    #   - main
    #   - stage
    paths:
      - 'backendAPI/**'
      - '.github/workflows/backendAPI.yml'
  # pull_request:
  #   paths:
  #     - 'backendAPI/**'


env:
  REGISTRY: ghcr.io
  COMMIT_SHA: ${{ github.event.after }}

jobs:
  test:
    runs-on: ubuntu-22.04
    environment: test
    env:
      KEYVAULT_HEALTH: ${{ vars.KEYVAULT_HEALTH }}
      POSTGRES_DB: ${{ vars.POSTGRES_DB }}
      POSTGRES_USER: ${{ secrets.POSTGRES_USER }}
      POSTGRES_PASSWORD: ${{ secrets.POSTGRES_PASSWORD }}
      REDIS_HOST: ${{ vars.REDIS_HOST }}
      REDIS_PORT: ${{ vars.REDIS_PORT }}
      MONGODB_HOST: ${{ vars.MONGODB_HOST }}
      MONGODB_PORT: ${{ vars.MONGODB_PORT }}
    steps:
      - name: echos different variables
        env:
          GITHUB_EVENT: ${{ toJSON(github.event) }}
        run: |
          echo "=== Is this the correct tag: github.event.after? ==="
          echo ${{ github.event.after }}
          echo "=== github.event.pull_request.head.sha ==="
          echo ${{ github.event.pull_request.head.sha }}
          echo "=== github.event.workflow_run.head_sha ==="
          echo ${{ github.event.workflow_run.head_sha }}
          echo "=== github.event.workflow_run.head_branch ==="
          echo ${{ github.event.workflow_run.head_branch }}
          echo "=== JSON deserialized github.event ==="
          echo $GITHUB_EVENT
          echo "=== GITHUB_SHA ==="
          echo $GITHUB_SHA
          echo "=== GITHUB_WORKFLOW_SHA ==="
          echo $GITHUB_WORKFLOW_SHA
          echo "=== github.sha ==="
          echo ${{ github.sha }}
      - uses: actions/checkout@v4
      - name: Build
        run: |
          docker compose \
            -f compose.yml \
            -f compose.override.test.yml \
            build backend_api
      - name: Code Formating
        run: |
          docker compose \
            -f compose.yml \
            -f compose.override.test.yml \
            run --rm backend_api \
            sh -c "black --check ."
      - name: Linting
        run: |
          docker compose \
            -f compose.yml \
            -f compose.override.test.yml \
            run --rm backend_api \
            sh -c "ruff check ."
      - name: Unit testing
        run: |
          docker compose \
            -f compose.yml \
            -f compose.override.test.yml \
            run --rm backend_api \
            sh -c "pytest -v"
  
  containerize:
    if: ${{ github.event.ref == 'refs/heads/main' || github.event.ref == 'refs/heads/stage' }}
    needs: test
    runs-on: ubuntu-22.04
    permissions:
      id-token: write
      contents: read
      packages: write
    environment: stage
    steps:
      # - name: echos github.event.after variable
      #   run: |
      #     echo "=== Is this the correct tag: github.event.after? ==="
      #     echo ${{ github.event.after }}
      #     echo $CONTAINER_TAG
      - uses: actions/checkout@v4
        with:
          ref: ${{ github.event.workflow_run.head_sha }} 
      - name: Login to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      - name: Build for production
        run: |
          docker compose -f compose.yml build --build-arg COMMIT_SHA=${{env.COMMIT_SHA}} backend_api
      - name: Show images
        run: docker image list
      - name: Tag and push with latest and commit hash
        run: |
          docker tag \
            ${{github.event.repository.name}}-backend_api:latest \
            ${{env.REGISTRY}}/${{github.repository}}-backend_api:latest
          docker tag \
            ${{github.event.repository.name}}-backend_api:latest \
            ${{env.REGISTRY}}/${{github.repository}}-backend_api:$COMMIT_SHA
          docker push \
            ${{env.REGISTRY}}/${{github.repository}}-backend_api:latest
          docker push \
              ${{env.REGISTRY}}/${{github.repository}}-backend_api:$COMMIT_SHA

  # postgres_migrations:
  #   needs: containerize
  #   runs-on: ubuntu-22.04
  # needs to run in azure container app environment, as it needs to access the database
  # add azure files as volume to store alembic.ini and migrations folder?
  # could run as a container app job? / at least needs to run in container app environment!
  # executes
  #
  # new ideas: run only
  # alembic check
  # in case it exits with other than 0 => write log as artifact
  # and trigger another workflow, that depolys a database admin container
  # and/or the current image to do following jobs:
  # - alembic revision --autogenerate -m "commit_id"
  # - get migrations script and commit back to git repo
  # - put migration scripts as artifacts?Â¿?
  # -     - name: Generate Alembic migration script
        #   run: |
        #   script=$(az containerapp exec --name <app-name> --resource-group <resource-group> --command "/bin/sh -c 'alembic revision --autogenerate -m \"commit_id\"'")
        #   echo "$script" > migration_script.py
        # - name: Commit and push migration script
        # run: |
        #   git config --global user.name 'GitHub Actions'
        #   git config --global user.email 'github-actions@github.com'
        #   git add migration_script.py
        #   git commit -m "Add migration script"
        #   git push
  # - manual approval step (show artifacts)?
  # - alembic upgrade head
  # then back to original script and run deploy_stage
  # same thing for prod OR reuse the migration scripts from stage?
  #
  #
  # a) create migration script:
  # - alembic revision --autogenerate -m "message"
  # az containerapp ... jobs ... docker compose ... alembic revision --autogenerate  -m "$github.event.after"
  #     message gets part of filename: use github.event.after AND/OR containertag? (should be same) AND/OR configre time_stemps for file_naming?
  # in between those steps include a manual approval step? So two jobs in github actions, where one gets a specific environment for approval?
  # just add the environment to the step - not the job! - no doesn't work. Need to add it to the job!
  # Well: a) is part of the code the migration versions can be commited to the repo, so no need for mounting a volume on the container app to persist the changes.
  # But what about the manual approval?
  # or check if migrations are necessary:
  # - alembic check
  #
  # b) review migration script:
    # that should be short and simple - just a manual approval step? => probably just referencing the migration environment in the following job is enough?
  #
  # c) run migration:
  # - alembic upgarde head
  #
  #
  # This hould be doable further up in the pipeline, as it is not dependent on the container app environment - only the codebase
  # Can even run somewhere after the tests and before the containerization - just needs to commit the changes to the repo, i.e. the migration scripts
  # putting them as artifacts somewhere and then downloading them in the containerize job?
  # jobs:
  #   generate_migrations:
  #     runs-on: ubuntu-latest
  #     steps:
  #       - name: Generate migration scripts
  #         run: |
  #           # Your commands to generate the migration scripts
  #           alembic revision --autogenerate -m "message"
  #       - name: Upload migration scripts
  #         uses: actions/upload-artifact@v2
  #         with:
  #           name: migrations
  #           path: /path/to/your/migration/scripts
  #
  #   review_migrations:
  #     needs: generate_migrations
  #     runs-on: ubuntu-latest
  #     environment: review
  #     steps:
  #       - name: Download migration scripts
  #         uses: actions/download-artifact@v2
  #         with:
  #           name: migrations
  #           path: /path/to/download/migration/scripts
  #       - name: Review migration scripts
  #         run: |
  #           # Your commands to review the migration scripts
  
  #   apply_migrations:
  #     needs: review_migrations
  #     runs-on: ubuntu-latest
  #     steps:
  #       - name: Download migration scripts
  #         uses: actions/download-artifact@v2
  #         with:
  #           name: migrations
  #           path: /path/to/download/migration/scripts
  #       - name: Apply migration scripts
  #         run: |
  #           # Your commands to apply the migration scripts
  #           alembic upgrade head

  deploy_stage:
    needs: containerize
    if: ${{ github.event.ref == 'refs/heads/main' || github.event.ref == 'refs/heads/stage' }}
    runs-on: ubuntu-22.04
    permissions:
      id-token: write
      packages: read
    environment: stage
    steps:
      # - name: echos github.event.after variable
      #   run: |
      #     echo "=== Is this the correct tag: github.event.after? ==="
      #     echo ${{ github.event.after }}
      #     echo $CONTAINER_TAG
      - name: Login to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      - name: Login to Azure
        uses: azure/login@v1
        with:
          client-id: ${{ secrets.AZURE_GITHUBACTIONSMANAGEDIDENTITY_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
      - name: Deploy to staging
      # TBD: changing mode to single here, as there is a terraform bug with the time-out, when setting containerapp to single!
        env:
          IDENTITY_REF: "/subscriptions/${{secrets.AZURE_SUBSCRIPTION_ID}}/resourcegroups/${{vars.AZURE_RESOURCE_GROUP}}/providers/Microsoft.ManagedIdentity/userAssignedIdentities/${{vars.AZURE_BACKENDIDENTITY_NAME}}"
        run: |
          az containerapp revision set-mode \
            --name ${{ vars.AZURE_CONTAINERAPP_BACKEND }} \
            --resource-group ${{vars.AZURE_RESOURCE_GROUP}} \
            --mode single
          az containerapp secret set \
            --name ${{ vars.AZURE_CONTAINERAPP_BACKEND }} \
            --resource-group ${{vars.AZURE_RESOURCE_GROUP}} \
            --secrets \
              "postgres-host=keyvaultref:${{ vars.AZURE_KEYVAULT_HOST }}/secrets/postgres-host,identityref:$IDENTITY_REF" \
              "keyvault-health=keyvaultref:${{ vars.AZURE_KEYVAULT_HOST }}/secrets/keyvault-health,identityref:$IDENTITY_REF" \
              "postgres-user=keyvaultref:${{ vars.AZURE_KEYVAULT_HOST }}/secrets/postgres-user,identityref:$IDENTITY_REF" \
              "postgres-password=keyvaultref:${{ vars.AZURE_KEYVAULT_HOST }}/secrets/postgres-password,identityref:$IDENTITY_REF"
          sleep 10
          az containerapp update \
            --name ${{ vars.AZURE_CONTAINERAPP_BACKEND }} \
            --resource-group ${{vars.AZURE_RESOURCE_GROUP}} \
            --image ${{env.REGISTRY}}/${{github.repository}}-backend_api:$COMMIT_SHA \
            --set-env-vars \
              "AZ_KEYVAULT_HOST=${{ vars.AZURE_KEYVAULT_HOST }}" \
              "KEYVAULT_HEALTH=secretref:keyvault-health" \
              "POSTGRES_HOST=secretref:postgres-host" \
              "POSTGRES_DB=${{ vars.POSTGRES_DB }}" \
              "POSTGRES_USER=secretref:postgres-user" \
              "POSTGRES_PASSWORD=secretref:postgres-password"
        # TBD: Run migrations here?
        # TBD: consider deleting all existing environment variables before setting the new ones?
        # implemented as in https://learn.microsoft.com/en-us/azure/container-apps/manage-secrets?tabs=azure-cli
        # TBD: consider putting all of this into a deploymentscript and reuse it and in deploy_prod!
      - name: Logout from Azure
        uses: azure/CLI@v1
        with:
          inlineScript: |
            az logout
            az cache purge
            az account clear

  check_postgres_migrations:
    needs: deploy_stage
    runs-on: ubuntu-22.04
    permissions:
      id-token: write
      # packages: read
    environment: stage
    steps:
      - name: Login to Azure
        uses: azure/login@v1
        with:
          client-id: ${{ secrets.AZURE_GITHUBACTIONSMANAGEDIDENTITY_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
      - name: Check for migrations
        id: check_run
        # OUTPUT=$(script -q -e -c "pwd")
        # OUTPUT=$(script -q -e -c "az containerapp exec \
        # --name ${{ vars.AZURE_CONTAINERAPP_BACKEND }} \
        # --resource-group ${{vars.AZURE_RESOURCE_GROUP}} \
        # --command \"scripts/postgres_migration_check.sh\""
        run: |
          mkdir -p postgres_migration
          script -q -e -c "az containerapp exec \
            --name ${{ vars.AZURE_CONTAINERAPP_BACKEND }} \
            --resource-group ${{vars.AZURE_RESOURCE_GROUP}} \
            --command \"scripts/postgres_migration_check.sh\"" > postgres_migration/check_${{ env.COMMIT_SHA }}.txt
          ALEMBIC_EXIT_CODE=$(grep "Alembic exit code:" postgres_migration/check_${{ env.COMMIT_SHA }}.txt | awk -F': ' '{print $2}')
          echo "Alembic exit code: $ALEMBIC_EXIT_CODE"
          echo "alembic_check_exit_code=$ALEMBIC_EXIT_CODE" >> $GITHUB_OUTPUT
      # TBD: remove if proven to work:
      # - name: Set elembic check exit code to 0 to check if call_migration is getting skipped
      #   run: |
      #     echo "alembic_check_exit_code=0" >> $GITHUB_OUTPUT
        # echo $OUTPUT
          # echo $EXIT_CODE
          # echo "OUTPUT=$OUTPUT" >> $GITHUB_OUTPUT
          # echo "EXIT_CODE=$EXIT_CODE" >> $GITHUB_OUTPUT
          # mkdir -p postgres_migration_check
          # echo $OUTPUT > postgres_migration_check/output_${{ env.COMMIT_SHA }}.txt
          # echo $EXIT_CODE > postgres_migration_check/exit_code_${{ env.COMMIT_SHA }}.txt
        # echo "::set-output name=postgres_migration_check_output::$OUTPUT"
        # echo "::set-output name=postgres_migration_check__exit_code::$EXIT_CODE"
        
      # - name: Safe migration check output to files
      #   run: |
      #     echo "${{ steps.check_run.outputs.postgres_migration_check_output }}" > ${{ github.workspace }}/postgres_migration_check/output.txt
      #     echo "${{ steps.check_run.outputs.postgres_migration_check_exit_code }}" > ${{ github.workspace }}/postgres_migration_check/exit_code.txt
      - name: Upload migration check artifacts
        uses: actions/upload-artifact@v3
        with:
          name: postgres_migration_results
          path: postgres_migration/check_${{ env.COMMIT_SHA }}.txt
      # - name: Display migration check output
      #   run: |
      #     echo ${{ steps.check_run.outputs.script_output }}
      #     echo ${{ steps.check_run.outputs.script_exit_code }}
      - name: Logout from Azure
        uses: azure/CLI@v1
        with:
          inlineScript: |
            az logout
            az cache purge
            az account clear
    outputs:
      alembic_check: ${{ steps.check_run.outputs.alembic_check_exit_code }}

  # call_migrations:
  #   needs: check_postgres_migrations
  #   uses: ./.github/workflows/postgres_migrations.yml
  #   # environment: COMMIT_SHA: ${{ env.COMMIT_SHA }}
  #   with:
  #     COMMIT_SHA: ${{ github.event.after }}
  #   secrets: inherit

  create_postgres_migration_scripts:
    needs: check_postgres_migrations
    if: ${{ needs.check_postgres_migrations.outputs.alembic_check != 0 }}
    runs-on: ubuntu-22.04
    permissions:
      id-token: write
      pull-requests: write
    environment: stage
    steps:
      - uses: actions/checkout@v4
      - name: Check what directory we are in
        # maybe we need a checkout repo step?
        run: |
          pwd
          ls -la
      - name: Echo COMMIT_SHA
        run: |
          echo ${{ env.COMMIT_SHA }}
          echo $COMMIT_SHA
      - name: Login to Azure
        uses: azure/login@v1
        with:
          client-id: ${{ secrets.AZURE_GITHUBACTIONSMANAGEDIDENTITY_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
      - name: Create migration scripts
        id: migration_scripts
        # OUTPUT=$(script -q -e -c "pwd")
        # OUTPUT=$(script -q -e -c "az containerapp exec \
        # --name ${{ vars.AZURE_CONTAINERAPP_BACKEND }} \
        # --resource-group ${{vars.AZURE_RESOURCE_GROUP}} \
        # --command \"scripts/postgres_migration_check.sh\""
        run: |
          mkdir -p postgres_migration
          script -q -e -c "az containerapp exec \
            --name ${{ vars.AZURE_CONTAINERAPP_BACKEND }} \
            --resource-group ${{vars.AZURE_RESOURCE_GROUP}} \
            --command \"scripts/postgres_migration_create.sh\"" > postgres_migration/create_${{ env.COMMIT_SHA }}.txt
      # TBD: another version below, that includes the migration run artifacts
      - name: Upload migration check artifacts
        uses: actions/upload-artifact@v3
        with:
          name: postgres_migration_results
          path: postgres_migration/create_${{ env.COMMIT_SHA }}.txt
      # Shall I really run it here already or first after pull request is merged?
      # No - let's run them right after deployment az containerapp update in stage & prod
      # - name: Run the migrations
      #   run: |
      #     mkdir -p postgres_migration_check
      #     script -q -e -c "az containerapp exec \
      #       --name ${{ vars.AZURE_CONTAINERAPP_BACKEND }} \
      #       --resource-group ${{vars.AZURE_RESOURCE_GROUP}} \
      #       --command \"scripts/postgres_migration_run.sh\"" > postgres_migration/run_$COMMIT_SHA.txt
      # - name: Upload migration check artifacts
      #   uses: actions/upload-artifact@v3
      #   with:
      #     name: postgres_migration_results
      #     path: |
      #       postgres_migration/create_${{ env.COMMIT_SHA }}.txt
      #       postgres_migration/run_${{ env.COMMIT_SHA }}.txt
      # If the following step fails, the migration script was not created correctly.
      # Probably line "/Generating <file_name>" is missing" - see artifacts!
      - name: Retrieve migration script from log
        run: |
          MIGRATION_SCRIPT_FILENAME=$(tail -n +4 postgres_migration/create_${{ env.COMMIT_SHA }}.txt | head -n 1)
          echo "---$MIGRATION_SCRIPT_FILENAME---"
          tail -n +5 postgres_migration/create_${{ env.COMMIT_SHA }}.txt | head -n -2 > $MIGRATION_SCRIPT_FILENAME.sql
          mv $MIGRATION_SCRIPT_FILENAME.sql backendAPI/src/migrations/versions
      - name: Echo the migration script
        run: |
          cat postgres_migration/create_${{ env.COMMIT_SHA }}.sql
      - name: Commit migration script
        uses: EndBug/add-and-commit@v9
        with:
          add: 'backendAPI/src/migrations/versions/create_${{ env.COMMIT_SHA }}.sql'
          message: '[migrations] Add migration script for commit ${{ env.COMMIT_SHA }}'
      - name: Echo git status
        run: |
          git status
      # - name: Create a pull request
      #   uses: peter-evans/create-pull-request@v5
      #   with:
      #     title: "[migrations] for postgres for commit ${{ env.COMMIT_SHA }}"
      #     commit-message: "[migrations] for commit ${{ env.COMMIT_SHA }}"
      #    # branch: "migrations/${{ env.COMMIT_SHA }}" really in doubt - let's see what comes out of add and commit?
      #     base: stage
      #     delete-branch: true
      #     labels: "postgres"
    
    # inputs are missing here!

      # This was just a dummy:
  # call_migrations:
  #   needs: check_postgres_migrations
  #   if: ${{ needs.check_postgres_migrations.outputs.alembic_check != 0 }}
  #   env: 
  #     ALEMBIC_CHECK_EXIT_CODE: ${{ needs.check_postgres_migrations.outputs.alembic_check }}
  #   runs-on: ubuntu-22.04
  #   permissions:
  #     id-token: write
  #     packages: read
  #   environment: stage
  #   steps:
  #     - name: Echo alembic check exit code
  #       run: |
  #         echo ${{ env.ALEMBIC_CHECK_EXIT_CODE }}
  #     - name: Download migration check artifacts
  #       uses: actions/download-artifact@v3
  #       with:
  #         name: postgres_migration_results
  #         path: postgres_migration
  #     - name: Display migration check output
  #       run: |
  #         cat postgres_migration/check_${{ env.COMMIT_SHA }}.txt
  #         echo $ALEMBIC_CHECK_EXIT_CODE
  #
  #
  #
      # call another workflow here, that
      # - builds the migration scripts
      # - commits them to the repo
      # - opens a pull request to repo
      # - on manual approval of pull request
      # - runs the migrations in stage

      # - name: Login to Azure
      #   uses: azure/login@v1
      #   with:
      #     client-id: ${{ secrets.AZURE_GITHUBACTIONSMANAGEDIDENTITY_CLIENT_ID }}
      #     tenant-id: ${{ secrets.AZURE_TENANT_ID }}
      #     subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
      # - name: Call migrations
      #   run: |
      #     script -q -e -c "az containerapp exec \
      #       --name ${{ vars.AZURE_CONTAINERAPP_BACKEND }} \
      #       --resource-group ${{vars.AZURE_RESOURCE_GROUP}} \
      #       --command \"scripts/postgres_migration_call.sh\""
      # - name: Logout from Azure
      #   uses: azure/CLI@v1
      #   with:
      #     inlineScript: |
      #       az logout
      #       az cache purge
      #       az account clear

# deploy_prod:
  # needs: [deploy-stage, check_postgres_migrations, migration-workflow]
  # if: |
  #   github.event.ref == 'refs/heads/main' &&
  #   (needs.check_postgres_migrations.outputs.alembic_check == '0' || needs.migration-workflow.result == 'success')
  # # ...

# No: the whole piepline is rerun on pull request from migrating database:
# deploy_prod:
  # needs: [deploy-stage, check_postgres_migrations
  # if: |
  #   github.event.ref == 'refs/heads/main' &&
  #   needs.check_postgres_migrations.outputs.alembic_check == '0'
  # if the migration workflow was not successful, the pull request did not get created!

  deploy_prod:
    needs: [ deploy_stage, check_postgres_migrations ]
    if: ${{ github.event.ref  == 'refs/heads/main' && needs.check_postgres_migrations.outputs.alembic_check == 0 }}
    # as this environmnet requires a manual review:
    # also wait on the pull request from postgres migrations to be merged
    # if check migrations detected changes.
    runs-on: ubuntu-22.04
    permissions:
      id-token: write
      packages: read
    environment: prod
    steps:
      # - name: echos github.event.after variable
      #   run: |
      #     echo "=== Is this the correct tag: github.event.after? ==="
      #     echo ${{ github.event.after }}
      #     echo $CONTAINER_TAG
      - name: Login to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      - name: Login to Azure
        uses: azure/login@v1
        with:
          client-id: ${{ secrets.AZURE_GITHUBACTIONSMANAGEDIDENTITY_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
      - name: Deploy to production
      # TBD: changing mode to single here, as there is a terraform bug with the time-out, when setting containerapp to single!
        env:
          IDENTITY_REF: "/subscriptions/${{secrets.AZURE_SUBSCRIPTION_ID}}/resourcegroups/${{vars.AZURE_RESOURCE_GROUP}}/providers/Microsoft.ManagedIdentity/userAssignedIdentities/${{vars.AZURE_BACKENDIDENTITY_NAME}}"
        # TBD: consider putting all of this into adeploymentscript and reuse her and in deploy_stage!
        run: |
          az containerapp revision set-mode \
            --name ${{ vars.AZURE_CONTAINERAPP_BACKEND }} \
            --resource-group ${{vars.AZURE_RESOURCE_GROUP}} \
            --mode single
          az containerapp secret set \
            --name ${{ vars.AZURE_CONTAINERAPP_BACKEND }} \
            --resource-group ${{vars.AZURE_RESOURCE_GROUP}} \
            --secrets \
              "postgres-host=keyvaultref:${{ vars.AZURE_KEYVAULT_HOST }}/secrets/postgres-host,identityref:$IDENTITY_REF" \
              "keyvault-health=keyvaultref:${{ vars.AZURE_KEYVAULT_HOST }}/secrets/keyvault-health,identityref:$IDENTITY_REF" \
              "postgres-user=keyvaultref:${{ vars.AZURE_KEYVAULT_HOST }}/secrets/postgres-user,identityref:$IDENTITY_REF" \
              "postgres-password=keyvaultref:${{ vars.AZURE_KEYVAULT_HOST }}/secrets/postgres-password,identityref:$IDENTITY_REF"
          sleep 10
          az containerapp update \
            --name ${{ vars.AZURE_CONTAINERAPP_BACKEND }} \
            --resource-group ${{vars.AZURE_RESOURCE_GROUP}} \
            --image ${{env.REGISTRY}}/${{github.repository}}-backend_api:$COMMIT_SHA \
            --set-env-vars \
              "AZ_KEYVAULT_HOST=${{ vars.AZURE_KEYVAULT_HOST }}" \
              "KEYVAULT_HEALTH=secretref:keyvault-health" \
              "POSTGRES_HOST=secretref:postgres-host" \
              "POSTGRES_DB=${{ vars.POSTGRES_DB }}" \
              "POSTGRES_USER=secretref:postgres-user" \
              "POSTGRES_PASSWORD=secretref:postgres-password"
        # TBD: consider deleting all existing environment variables before setting the new ones?
        # implemented as in https://learn.microsoft.com/en-us/azure/container-apps/manage-secrets?tabs=azure-cli
      # TBD: add running migration scripts - this should be the container,
      # - that worked in stage and has the latest scripts in the versions folder
      # - only alembic upgrade head should be necessary! (but maybe I can generate a new script and diff with the one from stage?)
      # - if that diff fails - roll immediately back to the previous revision?
      # TBD: consider displaying a "maintanance" page while the migrations are running
      # TBD: or consider creating separate migration scripts for stage and prod? => another pull request? 2 different directories?
      - name: Logout from Azure
        uses: azure/CLI@v1
        with:
          inlineScript: |
            az logout
            az cache purge
            az account clear